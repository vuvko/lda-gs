\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage[vlined,ruled,linesnumbered]{algorithm2e}

\begin{document}

\section{Определения и обозначения}
Имеется задача тематического моделирования, решаемая с помощью модели LDA (Latent Dirichlet Allocation). 
Используется реализация с~сэмплированием по~Гиббсу на~C++ --- GibbsLDA++ \cite{gibbslda++}. 
Для обучения в комплекте с библиотекой предоставляется приведённая к нужному формату данных коллекция новостей.

\subsection{Основные обозначения}
$ D $ --- коллекция текстовых документов. \newline
$ W $ --- словарь коллекции (всевозможные слова из документов).\newline
$ T $ --- множество тем.\newline
$ n_{dw} $ --- количество раз термин $ w \in W $ втретился в документе $ d \in D $.\newline
$\displaystyle n_d = \sum_{w\,\in\,d}n_{dw} $ --- длина документа $ d $.\newline
$\displaystyle n = \sum_{d\,\in\,D}\sum_{w\,\in\,d}n_{dw} $ --- длина всей коллекции.\newline
$ \alpha, \beta $ --- гиперпараметры распределения Дирихле.\newline
$\displaystyle \theta_{td} = p(t\,|\,d) $ --- распределение тем в~документе $ d $. Выбирается из распределения Дирихле с параметром $ \alpha $.\newline
$\displaystyle \varphi_{wt} = p(w\,|\,t) $ --- распределение слов в~теме $ t $. Выбирается из распределения Дирихле с параметром $ \beta $.

\subsection{Модель LDA}
Вероятностая модель LDA:
\[
  p(w\,|\,d) = \sum_{t \in T}p(w\,|\,t)p(t\,|\,d)
\]

Порождающая модель описана в алгоритме \ref{alg:genLDA}.

\begin{algorithm}
\label{alg:genLDA}
\caption{Порождающая модель LDA.}
\SetKwFor{For}{для всех}{}{конец}
\DontPrintSemicolon

  \For{$ t \in T $}{
    Выбрать распределение по словам: $ \varphi_{wt} \sim Dir_V(\beta) $\;
  }
  \For{$ d \in D $}{
    Выбрать распределение тем: $ \theta_{td} \sim Dir_K(\alpha) $\;
    \For{$ w \in d $}{
      Выбрать тему из распределения по данному документу: $ s \sim Cat_K(\theta_{td}) $\;
      Выбрать слово из распределения по взятой теме: $ w \sim Cat_V(\varphi_{ws}) $\;
    }
  }

\end{algorithm}

\section{Описание задачи}

Целью данного задания является ознакомление с тематическими моделями.
А также проверка адекватности нового внешнего критерия качества на модели LDA.

\subsection{Вычисление перплексии}

В качестве внутреннего критерия качества модели используюется перплексия, вычисляемая по формуле.

\[
  Perplexity(D) = 
    \exp\left(
      -\frac1{n}\sum_{d\,\in\,D} \sum_{w\,\in\,d} n_{dw} \ln\sum_{t\,\in\,T} \varphi_{wt} \theta_{td}
    \right)
\]

\subsection{Критерий качества по модельным данным}

Если известны исходные темы (например, обучение происходит на модельных данных), то после обучения можно будет попытаться найти соответствия между исходными и выданными алгоритмом темами.
Для нахождения соответствия логично ввести некотую меру сходства тем.
Будем считать сходство тем по следующей формуле:

\[
  \rho(t_{\,0},t) = 
    \sum_{w\in W}\left(
      \sqrt{p_{\,0}(w\,|\,t_{\,0})} - \sqrt{p\,(w\,|\,t)}
    \right)^2 + 
    \gamma\!\sum_{d\in D}\left(
      \sqrt{p_{\,0}(d\,|\,t_{\,0})} - \sqrt{p\,(d\,|\,t)}
    \right)^2
\]
$\displaystyle \gamma $ --- некоторый параметр. \newline
$\displaystyle p_{\,0}(w\,|\,t) $ --- <<реальное>> распределение слов по~темам. \newline
$\displaystyle p_{\,0}(d\,|\,t) $ --- <<реальное>> распределение документов по~тематикам, вычисляемое по~правилу Байеса.

Фактически, требуется решить задачу о назначениях.
Будем использовать Венгерский алгоритм для её решения.

Полученные венгерским алгоритмом соответствия позволяют ввести критерий качества для~модели, обученной на~модельных данных:

\[
R = \frac1{|T_0|} \sum_{t_{\,0}\in T_0} \rho(t_{\,0},V(t_{\,0}))
\]

$\displaystyle V(t_{\,0}) $ --- найденное Венгерским алгоритмом соответствие для темы $ t_{\,0} $.

\subsection{Эксперименты}

Для проведения экспериментов сгенерируем модельные данные по алгоритму \ref{alg:genData} с~параметрами: $ \alpha_0 $, $ \beta_{\,0} $ --- гиперпараметры распределения Дирихле, $ K_0 $ --- количество тем, $ \theta_{td}^{\,0} = p_{\,0}(t\,|\,d) \sim Dir_V(\alpha_0) $, $ \varphi_{wt}^0 = p_{\,0}(w\,|\,t) \sim Dir_{K_0}(\beta_{\,0}) $. 

\begin{algorithm}
\label{alg:genData}
\caption{Алгоритм генерации модельных данных.}
\SetKwFor{For}{для всех}{}{конец}
\DontPrintSemicolon

  \For{$ d\,\in\,D $}{
    \For{$ w\,\in\,d $}{
      Выбрать тему $ t \sim Cat_{K_)}(\theta_{td}^{\,0}) $\;
      Выбрать слово $ w \sim Cat_V(\varphi_{wt}^0) $\;
    }
  }

\end{algorithm}

%Построить графики перплексии и введённого критерия качества на каждой итерации обучения модели на сгенерированных данных.

На полученых модельных данных проведём несколько экспериментов:

\begin{enumerate}
  \item Начальные параметры модели ($ \alpha $, $ \beta $, $ K $) совпадают с параметрами модельной выборки.
  \item Неоднократный запуск обучения с одинаковыми параметрами (проверка устойчивости).
  \item Различные значения гиперпараметров $ \alpha $, $ \beta $ и количества тем $ T $.
\end{enumerate}

При проведении каждого из эксперимантов были построены графики перплексии и нового введёного критерия:

% Вставить графики

\subsection{Идеи повышения устойчивости модели}
\begin{itemize}
  \item Более точная оценка гиперпараметров $ \alpha $, $ \beta $.
  \item Разреженность $ \theta_{td} $ и $ \varphi_{wt} $. 
  Добавить этап разреживания в~алгоритм обучения модели. 
  Использовать различные параметры $ r_\theta $, $ r_\varphi $ -- доля нулевых элементов в $ \theta_{td} $ и $ \varphi_{wt} $ для~разреживания.
  % как лучше разреживать матрицы? Рандомно в зависимости от вероятности или лучше построить гистограмму и обрезать наименьшие?
\end{itemize}

% Проверить идеи и провести эксперименты

% 

\begin{thebibliography}{0}
\bibitem{gibbslda++}
  Реализация модели LDA с сэмплированием по~Гиббсу GibbsLDA++: \url{http://gibbslda.sourceforge.net/}

\bibitem{hungary_alg}
  Harold W. Kuhn, <<The Hungarian Method for the assignment problem>>, Naval Research Logistics Quarterly, 2:83–97, 1955.
\end{thebibliography}

\end{document}
